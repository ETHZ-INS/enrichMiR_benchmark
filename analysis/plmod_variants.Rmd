---
title: "enrichMiR plmod variants"
author: "tgermade"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: tango
    theme: cerulean
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
---


<style>
  .main-container {
    max-width: 1500px !important;
  }
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TLDR

## Questions to be answered:  

1a) Does a secondary *plmod* improve the scores we get from running an initial test on its own?  
1b) If so, is any of those better than the original *modSites* & *modScore*?  
2) Are the findings consistent across datasets?  
3) Does inclusion of logCPM weights improve performances?  
4a) Which variant performs best? OR Which variant to choose as default operation?  
4b) For each test: Which variant performs best?   

--> In other words: is it worth implementing a separate *plmod* variant based on any of the other tests? If so, which one?  

## Spoilers (new *co*):  

1a) We get a convergence of variants in terms of FP and in some cases of TP rate. For some tests this is beneficial for others less so (see [here](#overview)).   
1b) In all cases an additional primary test improves the *plmod* performance (see [here](#overview)).      
2) Yes (see [here](#exceptions)).     
3) Generally we get higher specificity without weights with a very slight decrease in sensitivity in some cases. The most drastic difference can be observed for the pure *plmod* performance, with a strong increase in specificity without weights (see [here](#weights)).  
4a) The rankings here are difficult. Higher focus on sensitivity: [*overlap (EN) bin_bs w/out weights*](#default). Higher focus on selectivity: [*siteoverlap (michael) w/out weights*](#default). Both tests are relatively fast, *overlap* more so (see [here](#runtimes)) (for a more in-depth look at runtimes see *enrichMiR_runtimes.html*).    
4b) Check [this table](#top_vars).   


## Spoilers (old *co*):  

A comparison of new vs. old *co*-selection method can be found [here](#co). Apart from that, only the new method is shown.  

1a) Yes, we get an increase in specificity in every case and an increase in sensitivity in some. Tests that on their own perform relatively badly benefit the most. When using *modSites* (binding-sites as variables: bs) we see a decrease in sensitivity in tests that have high specificity (see [here](#overview)).   
1b) Yes. We get an increase in specificity in all cases. Sensitivities however, are either equal or worse. The latter mostly in the *modSites* variants (bs) (see [here](#overview)).      
2) The tendency of decreased sensitivity for some variants is not discernible in the datasets *Amin*, *Jeong* and *Rat* (see [here](#exceptions)).     
3) When applied on tests that show high specificity, we get a small increase in sensitivity while specificity remains similar. On tests that show low specificity however, we get a large decrease in specificity with similar sensitivity. Summarized: Might be worth applying with high-specificity tests like *michael* aka. *siteoverlap*, *aREAmir* and some test aggregations (see [here](#weights)).  
4a) [*aREAmir bin_sc w/out weights*](#default). *aREAmir* is currently also our fastest test.  
4b) Check [this table](#top_vars).   

# Prep

```{r}
regBench <- function(file, tps, add.exp=FALSE){
  source("../functions/runBenchmark.R")
  res <- readRDS(file)
  # load different file to get experiment names... (ideally we already have this)
  if(add.exp){
    if(grepl("bartel|cherone", file)){
      n <- strsplit(rev(strsplit(file,split="\\/")[[1]]), split="\\.")[[1]][1:2]
      n <- paste(n, collapse=".")
    } else {
      n <- strsplit(rev(strsplit(file,split="\\/")[[1]]), split="\\.")[[1]][1]
    }
    n <- paste0("../results/", n, ".enrichMiR.rds")
    res.orig <- readRDS(n)
    names(res) <- names(res.orig)
    saveRDS(res, file)
  }
  # this is specifically for pure enrichMiR runs
  if(grepl("_pure",file) & !any(sapply(res[[1]],is.list))){
    for(i in names(res)){
      n.test <- sub("_pure","",rev(strsplit(file,split="\\/")[[1]])[2])
      tmp <- res[[i]]
      res[[i]] <- NULL
      res[[i]][[n.test]] <- tmp
      saveRDS(res, file)
    }
  }
  # do benchmark
  return( getBench(res, tps[names(res)], e.obj = FALSE) )
}
```

```{r}
TPs <- c( 
  let.7a = "GAGGUAG", lsy.6 = "UUUGUAU", miR.1 = "GGAAUGU", miR.124 = "AAGGCAC", 
  miR.137 = "UAUUGCU", miR.139 = "CUACAGU", miR.143 = "GAGAUGA", 
  miR.144 = "ACAGUAU", miR.153 = "UGCAUAG", miR.155 = "UAAUGCU", 
  miR.182 = "UUGGCAA", miR.199a = "CCAGUGU", miR.204 = "UCCCUUU", 
  miR.205 = "CCUUCAU", miR.216b = "AAUCUCU", miR.223 = "GUCAGUU", 
  miR.7 = "GGAAGAC", miR.122 = "GGAGUGU", miR.133 = "UGGUCCC", 
  miR.138 = "GCUGGUG", miR.145 = "UCCAGUU", miR.184 = "GGACGGA", 
  miR.190a = "GAUAUGU", miR.200b = "AAUACUG", miR.216a = "AAUCUCA", 
  miR.217 = "ACUGCAU", miR.219a = "GAUUGUC", miR.375 = "UUGUUCG", 
  miR.451a = "AACCGUU", "DKOvWT"="GCUACAU", "DKO"="GCUACAU", 
  "miR.138vNeg"="GCUGGUG", "miR.499vNeg"="UAAGACU", "miR.499"="UAAGACU", 
  "218DKOvWT"="UGUGCUU", "218DKO"="UGUGCUU",
  "138DKOvWT"="GCUGGUG", "138DKO"="GCUGGUG"
)
```

```{r}
getBenchLists <- function(path, pattern="\\.regmir\\."){
  files <- list.files(path, pattern=pattern, full.names=TRUE)
  d.list <- lapply(files, function(file) regBench(file, TPs))
  
  n.files <- sapply(files, function(f){
    if(grepl("bartel|cherone",f)){
      n <- strsplit(rev(strsplit(f,split="\\/")[[1]]), split="\\.")[[1]][-3]
    } else {
      n <- strsplit(rev(strsplit(f,split="\\/")[[1]]), split="\\.")[[1]][-2]
    }
    paste(n, collapse=".")
    })
  names(d.list) <- n.files
  return(d.list)
}
```

```{r}
# load plmod test combinations
d.list <- getBenchLists(path="../results_plmod2", pattern="\\.plmod\\.")
# load plmod pure
d.list2 <- getBenchLists(path="../results_plmod2/plmod_pure", pattern="\\.plmod\\.")
```

```{r}
df <- dplyr::bind_rows(d.list, .id="id")
df <- df[!grepl("modS",df$method),]
df2 <- dplyr::bind_rows(d.list2, .id="id")
```

```{r}
# add original test results
originals <- c("../results/bartel.hek.benchmark.rds",
               "../results/bartel.hela.benchmark.rds",
               "../results/amin.benchmark.rds",
               "../results/jeong.benchmark.rds",
               "../results/ratPolyA.benchmark.rds",
               "../results/cherone.d0.benchmark.rds",
               "../results/cherone.d1.benchmark.rds")
d.list3 <- lapply(originals,readRDS)
names(d.list3) <- sapply(originals, function(x) rev(strsplit(x,split="\\/")[[1]])[1])
df3 <- dplyr::bind_rows(d.list3, .id="id")
df3 <- df3[df3$prop=="original",]
```

```{r}
# combine all benchmarks
df <- rbind(df, df2, df3[,-c(3,10)])
```

```{r}
# prep
df$dataset <- "bartel.hela"
df$dataset[grepl("bartel.hek",df$id)] <- "bartel.hek"
df$dataset[grepl("amin",df$id)] <- "amin"
df$dataset[grepl("jeong",df$id)] <- "jeong"
df$dataset[grepl("ratPolyA",df$id)] <- "rat"
df$dataset[grepl("cherone.d0",df$id)] <- "cherone.d0"
df$dataset[grepl("cherone.d1",df$id)] <- "cherone.d1"
df$mod.var <- sapply(df$id, function(x){
  if(grepl("benchmark",x)){
    "original"
  } else {
    if(grepl("bartel|cherone",x)){
      strsplit(x, split="\\.")[[1]][3]
    } else {
      strsplit(x, split="\\.")[[1]][2]
    }
  }
})
df$weight <- "logCPM"
df$weight[!grepl("\\.w\\.",df$id)] <- "NULL"

w.up <- grepl("\\.up",df$method) 
df <- df[!(w.up & df$dataset!="jeong"),]
w.down <- grepl("\\.down",df$method)
df <- df[!(w.down & df$dataset=="jeong"),]
df$method <- gsub("\\.up","",gsub("\\.down","",df$method))
```

# Results

```{r, warning=F,message=F}
source("../functions/bmPlots.R")
```

![In all following instances *plmod* refers to pure *plmod* results (not *plmod* results with additional linear regression!).](../archive/plmod_explained.png){#id .class width=80% height=80%}  

## Overview {#overview}

```{r, warning=F,message=F, fig.width=9}
# overview: compare new plmod to original tests
TPvFP(df, features=c("mod.var"), labels=c("plmod","modScore","modSites"))
```

## Each dataset separately {#exceptions}

```{r, warning=F,message=F, fig.height=7, fig.width=9}
# compare plmod variants
TPvFP(df[grepl("bartel",df$dataset),], features=c("mod.var","dataset"), labels=c("plmod","modScore","modSites"))
```
```{r, warning=F,message=F, fig.height=5, fig.width=9}
TPvFP(df[grepl("amin",df$dataset),], features=c("mod.var","dataset"), labels=c("plmod","modScore","modSites"))
```
```{r, warning=F,message=F, fig.height=5, fig.width=9}
TPvFP(df[grepl("jeong",df$dataset),], features=c("mod.var","dataset"), labels=c("plmod","modScore","modSites"))
```
```{r, warning=F,message=F, fig.height=5, fig.width=9}
TPvFP(df[grepl("rat",df$dataset),], features=c("mod.var","dataset"), labels=c("plmod","modScore","modSites"))
```
```{r, warning=F,message=F, fig.height=5, fig.width=9}
TPvFP(df[grepl("cherone",df$dataset),], features=c("mod.var","dataset"), labels=c("plmod","modScore","modSites"))
```

## Effect of weights {#weights}

Here the original test results (from thesis) are excluded.  

```{r, warning=F,message=F, fig.width=9}
# check effect of weights
TPvFP(df[df$mod.var!="original",], label.all=TRUE, features=c("weight"))
```
```{r, warning=F,message=F, fig.height=7, fig.width=9}
# look at the effect of weights on each of the separate input types
TPvFP(df, features=c("mod.var","weight"), labels=c("plmod","modScore","modSites"))
```

## Top 10 % performing variants

The higher the following ratios the better. They represent the combined rankings of TP and FP of *plmod* variants among the top 10%.  

Note that we get the variants that contain the instances/tests with the highest TP & FP scores. We don't get the variants that get the best scores averaged over all instances. This means that even though here we see *bin_sc* performing best, the plots above suggest that *cont_sc* performs best over all tests.     
### Variants

```{r, warning=F,message=F}
# rank plmod variants based on TP & FP
ag <- aggregate(df[,c("FP.atFDR05","TP.atFDR05","detPPV")], by=df[,c("mod.var","method","weight")], FUN=mean, na.rm=T)

rank <- data.frame(FP=rownames(ag[order(ag$FP.atFDR05, ag$detPPV, decreasing=c(F,T)),]),
                   TP=rownames(ag[order(ag$TP.atFDR05, ag$detPPV, decreasing=TRUE),])
                   )
rank <- sapply(1:nrow(rank), function(i) which(rank$FP==i) + which(rank$TP==i))
ag <- ag[order(rank),]
```

For each variant, what's the ratio of total measured instances of this variant in the top 10%?  

```{r, warning=F,message=F}
# ratio of plmod instances in the top 10% ranks [vs. individual sizes of each plmod variant]
round( table(ag$mod.var[1:floor(nrow(ag)/10)]) / table(ag$mod.var)[-c(2,4)] ,2)
```

How the top 10% are split up in terms of variants. When summed, they give 100%.  

```{r, warning=F,message=F}
# ratio of plmod instances in the top 10% ranks [vs. number of ranks that make up the top 10%]
round( table(ag$mod.var[1:floor(nrow(ag)/10)]) / sum(table(ag$mod.var[1:floor(nrow(ag)/10)])) ,2)
```

### Weights  

For each variant, what's the ratio of total measured instances of this variant in the top 10%?  

```{r, warning=F,message=F}
# ratio of weight variants in the top 10% ranks [vs. individual sizes of each weight variant]
round( table(ag$weight[1:floor(nrow(ag)/10)]) / table(ag$weight) ,2)
```

How the top 10% are split up in terms of variants. When summed, they give 100%.  

```{r, warning=F,message=F}
# ratio of weight variants in the top 10% ranks [vs. number of ranks that make up the top 10%]
round( table(ag$weight[1:floor(nrow(ag)/10)]) / sum(table(ag$weight[1:floor(nrow(ag)/10)])) ,2)
```

Sum of ranks per variant over all runs (not just top 10%):  

```{r, warning=F,message=F}
# sum of ranks for weight variants (the smaller the number the better)
c(logCPM=sum(which(ag$weight=="logCPM")), `NULL`=sum(which(ag$weight=="NULL")))
```

Looking at the top 10% performing tests, we see a very clear advantage for no_weights. Looking at all ranks combined, separated by weight variants, we see a very slight advantage for weights. Should we implement defaults for every primary test option individually? Or simply use no weights?  

### Tables {#default}  

Here the rankings can be a bit deceiving: The binary signal variants (bin_bs) are very highly ranked in comparison to the continuous ones (cont_bs). When comparing the two, cont_bs has both a lower FP & TP rate as well as a higher detPPV. We need to ask ourselves if we want to focus on sensitivity or selectivity.  

Table of top 10% performing variants:  

```{r, warning=F,message=F}
# top ranks of aggregated table
head(ag, floor(nrow(ag)/10))
```

Here the top 5 variants for bin_bs & cont_bs:  

```{r}
head(ag[ag$mod.var=="bin_bs",], 5)
head(ag[ag$mod.var=="cont_bs",], 5)
```

## Top variant for each test {#top_vars}  

```{r}
ag2 <- ag[ag$mod.var!="original",]
do.call(rbind, lapply(unique(ag2$method), function(x) ag2[ag2$method==x,][1,]))
```

## Heatmaps

```{r, warning=F,message=F, fig.height=6, fig.width=7}
HM(df, form=method~mod.var+weight, cluster.cols = T)
HM(df, metric="FP.atFDR05", form=method~mod.var+weight, cluster.cols = T)
```

## Runtime Analysis {#runtimes}  

We used to define *co* based on the miRNAs that pass as significant. This meant *co* lengths and thus the number of variables that were tested in a secondary test (here *regmir*) differed for every primary test (i.e. using *regmir* as primary test might result in a smaller *co* than using *areamir*). Now we define *co* based on a top percentage of miRNAs (in terms of FDR & pvalue). This means that *co* lengths are largely identical for all tests. This means that we expect to see smaller differences in the performances between each variant (defined by their different primary tests).  

These are the codes for both versions:  

```{r, eval=FALSE}
# old: 
if( sum(e$FDR<=.05,na.rm=TRUE) > 0 ){
  co <- rownames(e[e$FDR<=.05,])
} else if( sum(e$FDR<=.2) > 0 ){
  co <- rownames(e[e$FDR<=.2,])
} else if( nrow(e)>=10 ) {
  co <- rownames(e[1:10,])
} else {
  co <- rownames(e[1:nrow(e),])
}
```

```{r, eval=FALSE}
# new:
if( nrow(e) >= 5 ){
  co <- head(rownames(e),n=max(5,ceiling(nrow(e)/10)))
} else {
  co <- rownames(e)
}
```

### Time for each test individaully

```{r, warning=F, message=F}
rts <- readRDS("../results_runtime/combined.runtimes.rds")
names(rts) <- nd <- c("amin","bartel.hek","bartel.hela","jeong")
```

```{r}
formDF <- function(l){
  df <- as.data.frame(dplyr::bind_rows(l, .id="id"))
  df2 <- stack(df[!colnames(df) %in% "id"])
  df2$id <- factor(df$id)
  df2$test <- factor(rownames(l[[1]]))
  return(df2)
}
```

```{r}
rdf1 <- formDF(rts[nd[c(1,4)]])
rdf2 <- formDF(rts[nd[-c(1,2,4)]])
rdf3 <- formDF(rts[nd[-c(1,3,4)]])
```

```{r, warning=FALSE, message=FALSE}
# combine runtimes of all datasets
rdf <- rbind(rdf1,rdf2,rdf3)
ag <- aggregate(rdf[,"values"], by=list(rdf[,"test"]), FUN=mean)
nt <- ag[,1]
ag <- ag[,-1]
names(ag) <- nt
round(ag[order(ag)],1)
```

All values are in seconds per run. The *areamir* test is by far the fastest. The new *plmod* code speeds it up, however not as much as first expected. A more detailed analysis of runtimes can be found in *enrichMiR_runtimes.html*.  

# Comparing old & new variable selection (*co*) {#co}     

```{r}
# load regmir test combinations
od.list <- getBenchLists(path="../results_plmod/", pattern="\\.plmod\\.")
# load regmir pure
od.list2 <- getBenchLists(path="../results_plmod/plmod_pure/", pattern="\\.plmod\\.")
```

```{r}
odf <- dplyr::bind_rows(od.list, .id="id")
odf <- odf[!grepl("modS",odf$method),]
odf2 <- dplyr::bind_rows(od.list2, .id="id")
```

```{r}
odf <- rbind(odf, odf2)
```

```{r}
# prep
odf$dataset <- "bartel.hela"
odf$dataset[grepl("bartel.hek",odf$id)] <- "bartel.hek"
odf$dataset[grepl("amin",odf$id)] <- "amin"
odf$dataset[grepl("jeong",odf$id)] <- "jeong"
odf$dataset[grepl("ratPolyA",odf$id)] <- "rat"
odf$dataset[grepl("cherone.d0",odf$id)] <- "cherone.d0"
odf$dataset[grepl("cherone.d1",odf$id)] <- "cherone.d1"
odf$mod.var <- sapply(odf$id, function(x){
  if(grepl("benchmark",x)){
    "original"
  } else {
    if(grepl("bartel|cherone",x)){
      strsplit(x, split="\\.")[[1]][3]
    } else {
      strsplit(x, split="\\.")[[1]][2]
    }
  }
})
odf$weight <- "logCPM"
odf$weight[!grepl("\\.w\\.",odf$id)] <- "NULL"

w.up <- grepl("\\.up",odf$method) 
odf <- odf[!(w.up & odf$dataset!="jeong"),]
w.down <- grepl("\\.down",odf$method)
odf <- odf[!(w.down & odf$dataset=="jeong"),]
odf$method <- gsub("\\.up","",gsub("\\.down","",odf$method))
```

```{r}
df$co <- "new_co"
odf$co <- "old_co"
combdf <- rbind(df,odf)
```

## all variants  

```{r, warning=F,message=F, fig.width=9}
# check effect of different feature selection (co) in logistic models
TPvFP(combdf[combdf$mod.var!="original",], label.all=TRUE, features=c("co"))
```

The old *co* is very different for each variant. The new one is by definition very similar among them: It takes the top 10% miRNAs and feeds them into the secondary regression. It is unsurprising that the spread of the results is thus much more narrow. It also means that the *co*s are larger in most cases, leading to a decrease in specificity for with an increase in sensitivity in some cases. Depending on the selectivity we want to achieve it might be worth doing further analyses with a slightly smaller percentage of miRNAs selected for *co*. The new method definitely makes many more primary tests viable and preferrable over a pure *plmod*. Note however, that we are still dealing with a median of `r median(df[df$mod.var!="original",]$FP.atFDR05)` FPs over all methods and datasets.   

## modScore variants  

```{r, warning=F,message=F, fig.width=9}
# check effect of different feature selection (co) in logistic models
TPvFP(combdf[combdf$mod.var %in% c("bin_sc","cont_sc"),], label.all=TRUE, features=c("co"))
```

Here we only look at the variants using scores as variables, which performed best in most of our metrics. We no longer see the stark improvement in sensitivities. Instead all we see is a convergence of results which leads to a decrease in performance for those tests that previously ranked highest.  

# Comparing old & new pure plmod runs

The original plmod runs should be *cont_bs* (modSites) & *cont_sc* (modScore) w/out weights. See here:  

```{r, eval=FALSE}

# enrichMiR.R

## calls regmir & regmirb
if(is.null(tests) || "modsites" %in% tests) o@res$modSites=plMod(DEA, TS, minSize, var="sites", correctForLength=T)
if(is.null(tests) || "modscore" %in% tests) o@res$modScore=plMod(DEA, TS, minSize, var="score", correctForLength=F)

# tests.R: plMod()

## signal:
fcs <- dea$logFC
if(is.null(cfl)){
      mod <- try(lm(fcs~x2+0),silent=T)
    }else{
      mod <- try(lm(fcs~x2+cfl+0),silent=T)
    }
## --> signals are continuous in both cases (modScore & modSites)

## variables
res <- t(sapply(split(TS,TS$family), ..., FUN=function(x,fcs, ...){
  ...
  x2 <- x[names(fcs),var]
  ...
}
## --> variables are continuous in both cases (either scores or binding-sites)
```

We expect 2 *plmod* variants to correspond to the original *modSites* & *modScores* runs. Do we find any instances of identical metrics between 2 versions?  

```{r, warning=F,message=F}
# do we find instances with the exact same results btw. old & new regmir runs?
old <- df[grepl("modS",df$method) & df$mod.var=="original",]
old <- old[,c("detPPV","FP.atFDR05","log10QDiff","TP.atFDR05")]
new <- df[df$method=="plmod" & df$mod.var!="original",]
new <- new[,c("detPPV","FP.atFDR05","log10QDiff","TP.atFDR05")]
any(
  sapply(1:nrow(new), function(i){
    any( sapply(1:nrow(old), function(j) identical(new[i,], old[j,])) )
    })
)
```

No we don't. How do the metric distributions look like when comparing old & new version?   

```{r, warning=F,message=F, fig.width=4, fig.height=3}
new <- df[df$method=="plmod" & df$mod.var %in% c("cont_bs","cont_sc") & 
            df$weight=="NULL",]
new <- new[,c("detPPV","FP.atFDR05","log10QDiff","TP.atFDR05")]
test <- do.call(rbind, list(old, new))
test$id <- c(rep("old",nrow(old)),rep("new",nrow(new)))

ggplot(test, aes(log10QDiff, col=id)) + geom_freqpoly()
ggplot(test, aes(detPPV, col=id)) + geom_freqpoly()
ggplot(test, aes(TP.atFDR05, fill=id)) + geom_bar(position = "dodge")
ggplot(test, aes(FP.atFDR05, col=id)) + geom_freqpoly()
```

They're highly similar but not identical.  

